{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic (A2C) on CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor-critic is an algorithm that combines both policy gradient (the actor) and value function (the critic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A2C](imgs/Advantage_actor_critic.png)\n",
    "Credit: Sergey Levine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C is a more sophisticated version of the actor-critic that use the advantage, n-step return and a policy is run in multiple (synchronous) environments. \n",
    "[A3C](https://arxiv.org/pdf/1602.01783.pdf) is an asynchronous A2C with the environments that are run in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Actor and Critic can share the same neural network or have two separate network design. In this example, I used a shared network.\n",
    "<img src=\"imgs/nn_ac.png\" alt=\"drawing\" width=\"600\"/>\n",
    "Credit: Sergey Levine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.clip_grad import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C_nn(nn.Module):\n",
    "    '''\n",
    "    Advantage actor-critic neural net\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(A2C_nn, self).__init__()\n",
    "\n",
    "        self.lp = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], 64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.policy = nn.Linear(64, n_actions)\n",
    "        self.value = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l = self.lp(x.float())\n",
    "        # return the actor and the critic\n",
    "        return self.policy(l), self.value(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total loss contains:\n",
    "- actor loss $\\partial\\theta_v\\leftarrow\\partial\\theta_v + \\dfrac{\\partial(R-V_\\theta(s))^2}{\\partial\\theta_v}$\n",
    "- policy loss $\\partial\\theta_\\pi\\leftarrow\\partial\\theta_\\pi + \\alpha\\triangledown_\\theta log\\pi_\\theta(a|s)(R-V_\\theta(s))$\n",
    "- entropy loss $\\beta\\sum_i\\pi_\\theta(s)log\\pi_\\theta(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(memories, nn, writer):\n",
    "    '''\n",
    "    Calculate the loss of the memories\n",
    "    '''\n",
    "\n",
    "    #batch_mem = np.random.choice(len(memories), size=32)\n",
    "\n",
    "    rewards = torch.tensor(np.array([m.reward for m in memories], dtype=np.float32))\n",
    "    log_val = nn(torch.tensor(np.array([m.obs for m in memories], dtype=np.float32)))\n",
    "\n",
    "    act_log_softmax = F.log_softmax(log_val[0], dim=1)[:,np.array([m.action for m in memories])]\n",
    "    # Calculate the advantage\n",
    "    adv = (rewards - log_val[1].detach())\n",
    "\n",
    "    # actor loss (policy gradient)\n",
    "    pg_loss = - torch.mean(act_log_softmax * adv)\n",
    "    # critic loss (value loss)\n",
    "    vl_loss = F.mse_loss(log_val[1].squeeze(-1), rewards)\n",
    "    # entropy loss\n",
    "    entropy_loss = ENTROPY_BETA * torch.mean(torch.sum(F.softmax(log_val[0], dim=1) * F.log_softmax(log_val[0], dim=1), dim=1))\n",
    "\n",
    "    # total loss\n",
    "    loss = pg_loss + vl_loss - entropy_loss\n",
    "\n",
    "    # add scalar to the writer\n",
    "    writer.add_scalar('loss', float(loss), n_iter)\n",
    "    writer.add_scalar('pg_loss', float(pg_loss), n_iter)\n",
    "    writer.add_scalar('vl_loss', float(vl_loss), n_iter)\n",
    "    writer.add_scalar('entropy_loss', float(entropy_loss), n_iter)\n",
    "    writer.add_scalar('actions', np.mean([m.action for m in memories]), n_iter)\n",
    "    writer.add_scalar('adv', float(torch.mean(adv)), n_iter)\n",
    "    writer.add_scalar('act_lgsoft', float(torch.mean(act_log_softmax)), n_iter)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    '''\n",
    "    Environment class. Used to deal with multiple environments\n",
    "    '''\n",
    "\n",
    "    game_rew = 0\n",
    "    last_game_rew = 0\n",
    "\n",
    "    def __init__(self, env_name, n_steps, gamma):\n",
    "        super(Env, self).__init__()\n",
    "\n",
    "        # create the new environment\n",
    "        self.env = gym.make(env_name)\n",
    "        self.obs = self.env.reset()\n",
    "\n",
    "        self.n_steps = n_steps\n",
    "        self.action_n = self.env.action_space.n\n",
    "        self.observation_n = self.env.observation_space.shape[0]\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def step(self, agent):\n",
    "        '''\n",
    "        Execute the agent n_steps in the environment\n",
    "        '''\n",
    "        memories = []\n",
    "        for s in range(self.n_steps):\n",
    "\n",
    "            # get the agent policy\n",
    "            pol_val = agent(torch.tensor(self.obs))\n",
    "            s_act = F.softmax(pol_val[0])\n",
    "\n",
    "            # get an action following the policy distribution\n",
    "            action = int(np.random.choice(np.arange(self.action_n), p=s_act.detach().numpy(), size=1))\n",
    "\n",
    "            # Perform a step in the environment\n",
    "            new_obs, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            # update the memory\n",
    "            memories.append(Memory(obs=self.obs, action=action, new_obs=new_obs, reward=reward, done=done))\n",
    "\n",
    "            self.game_rew += reward\n",
    "            self.obs = new_obs\n",
    "\n",
    "            if done:\n",
    "                # if done reset the env and the variables\n",
    "                self.done = True\n",
    "                # if the game is over, run_add take the 0 value\n",
    "                self.run_add = 0\n",
    "                self.obs = self.env.reset()\n",
    "\n",
    "                self.last_game_rew = self.game_rew\n",
    "                self.game_rew = 0\n",
    "                break\n",
    "            else:\n",
    "                self.done = False\n",
    "\n",
    "        if not self.done:\n",
    "            # if the game isn't over, run_add take the value of the last state\n",
    "            self.run_add = float(agent(torch.tensor(self.obs))[1])\n",
    "\n",
    "        # compute the discount reward of the memories and return it\n",
    "        return self.discounted_rewards(memories)\n",
    "\n",
    "\n",
    "    def discounted_rewards(self, memories):\n",
    "        '''\n",
    "        Compute the discounted reward backward\n",
    "        '''\n",
    "        upd_memories = []\n",
    "\n",
    "        for t in reversed(range(len(memories))):\n",
    "            if memories[t].done: self.run_add = 0\n",
    "            self.run_add = self.run_add * self.gamma + memories[t].reward\n",
    "\n",
    "            # Update the memories with the discounted reward\n",
    "            upd_memories.append(Memory(obs=memories[t].obs, action=memories[t].action, new_obs=memories[t].new_obs, reward=self.run_add, done=memories[t].done))\n",
    "\n",
    "        return upd_memories[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Memory = namedtuple('Memory', ['obs', 'action', 'new_obs', 'reward', 'done'], rename=False)\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.003\n",
    "ENTROPY_BETA = 0.2\n",
    "ENV_NAME = 'LunarLander-v2'\n",
    "\n",
    "MAX_ITER = 100000\n",
    "# Number of the env\n",
    "N_ENVS = 40\n",
    "\n",
    "# Max normalized gradient\n",
    "CLIP_GRAD = 0.1\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "date_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, now.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-98a7754fcfb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create N_ENVS environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_ENVS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content/runs/A2C'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdate_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-98a7754fcfb2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create N_ENVS environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_ENVS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content/runs/A2C'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdate_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Env' is not defined"
     ]
    }
   ],
   "source": [
    "# create N_ENVS environments\n",
    "envs = [Env(ENV_NAME, 1, GAMMA) for _ in range(N_ENVS)]\n",
    "\n",
    "writer = SummaryWriter(log_dir='content/runs/A2C'+ENV_NAME+'_'+date_time)\n",
    "\n",
    "# initialize the actor-critic NN\n",
    "agent_nn = A2C_nn(gym.make(ENV_NAME).observation_space.shape, gym.make(ENV_NAME).action_space.n).to(device)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = optim.Adam(agent_nn.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
    "\n",
    "experience = []\n",
    "n_iter = 0\n",
    "\n",
    "while n_iter < MAX_ITER:\n",
    "    n_iter += 1\n",
    "\n",
    "    # list containing all the memories\n",
    "    memories = [mem for env in envs for mem in env.step(agent_nn)]\n",
    "\n",
    "    # calculate the loss\n",
    "    losses = calculate_loss(memories, agent_nn, writer)\n",
    "\n",
    "    # optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    # clip the gradient\n",
    "    clip_grad_norm_(agent_nn.parameters(), CLIP_GRAD)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    writer.add_scalar('rew', np.mean([env.last_game_rew for env in envs]), n_iter)\n",
    "    print(n_iter, np.round(float(losses),2), 'rew:', np.round(np.mean([env.last_game_rew for env in envs]),2))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION! the model is not working, look at the graph below. Why this strange behavior? I tried to tune the hyperparameters but the results are the same.\n",
    "![Reward plot](imgs/reward_plot_a2c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is the loss decreasing so fast? \n",
    "![Reward plot](imgs/loss_plot_a2c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In some cases, the model start preferring always the same action..\n",
    "![Reward plot](imgs/actions_plot_a2c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some idea:\n",
    " - Use two different neural networks and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-1566e77ac676>, line 104)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-1566e77ac676>\"\u001b[0;36m, line \u001b[0;32m104\u001b[0m\n\u001b[0;31m    if (np.random.random() <= max(0.01, 0.9*(len(self.train_rewards)**0.9)))):\u001b[0m\n\u001b[0m                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "#import ipdb\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "print(\"use_cuda : \", use_cuda)\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, num_actions):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(16, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x),dim=-1)\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 16)\n",
    "        self.dp1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        #self.fc3 = nn.Linear(16, 16)\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dp1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class A2C(object):\n",
    "    def __init__(self, env, args):\n",
    "        super(A2C, self).__init__()\n",
    "        self.env = env\n",
    "        self.actor = Actor(env.observation_space.shape[0], env.action_space.n)\n",
    "        self.critic = Critic(env.observation_space.shape[0])\n",
    "        if use_cuda:\n",
    "            self.actor.cuda()\n",
    "            self.critic.cuda()\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=args.lr_actor)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=args.lr_critic)\n",
    "        self.N_steps = args.N_steps\n",
    "        self.num_episodes = args.num_episodes\n",
    "        self.test_episodes = args.test_episodes\n",
    "        #self.num_steps = args.num_steps\n",
    "        self.gamma = args.gamma\n",
    "        self.expt_name = args.expt_name\n",
    "        self.save_path = args.save_path\n",
    "        self.test_freq = args.test_freq\n",
    "        self.save_freq = args.save_freq\n",
    "        self.train_rewards = []\n",
    "        self.test_rewards = []\n",
    "        self.train_steps = []\n",
    "        self.test_steps = []\n",
    "        self.losses_actor = []\n",
    "        self.losses_critic = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = Variable(Tensor(state))\n",
    "        log_probs = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = Categorical(log_probs.exp()).sample()\n",
    "        return action.data.cpu().item(), log_probs[action], value\n",
    "\n",
    "    def play_episode(self, e):\n",
    "        state = self.env.reset()\n",
    "        steps = 0\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        # for i in range(self.num_steps):\n",
    "        while True:\n",
    "            action, log_prob, value = self.select_action(state)\n",
    "            if (np.random.random() <= max(0.01, 0.9*(len(self.train_rewards)**0.9)))):\n",
    "                action = env.action_space.sample() \n",
    "#             print(action, log_prob)\n",
    "            state, reward, is_terminal, _ = self.env.step(action)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            steps +=1\n",
    "            if is_terminal or steps >= 900:\n",
    "#                 if steps>=900:\n",
    "#                     rewards[-1]=-10000\n",
    "                break\n",
    "#         print(log_probs)\n",
    "        a = torch.stack(log_probs)\n",
    "        b = torch.stack(values)\n",
    "        return steps, rewards, a,b\n",
    "\n",
    "    def optimize(self, rewards, log_probs, values):\n",
    "        T = len(rewards)\n",
    "        N = self.N_steps\n",
    "        R = np.zeros(T, dtype=np.float32)\n",
    "        loss_actor = 0\n",
    "        loss_critic = 0\n",
    "        for t in reversed(range(T)):\n",
    "            V_end = 0 if (t+N >= T) else values[t+N].data\n",
    "            R[t] = (self.gamma**N * V_end) + sum([self.gamma**k * rewards[t+k]*1e-2 for k in range(min(N, T-t))])\n",
    "        R = Variable(Tensor(R), requires_grad=False)\n",
    "        # compute losses using the advantage function;\n",
    "        # Note: `values` is detached while computing loss for actor\n",
    "        loss_actor = ((R - values.detach()) * -log_probs).mean()\n",
    "        loss_critic = ((R - values)**2).mean()\n",
    "        # loss = loss_actor + loss_critic\n",
    "\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        loss_critic.backward()\n",
    "        # nn.utils.clip_grad_norm(self.actor.parameters(), grad_norm_limit)\n",
    "        # nn.utils.clip_grad_norm(self.critic.parameters(), grad_norm_limit)\n",
    "        self.optimizer_actor.step()\n",
    "        self.optimizer_critic.step()\n",
    "        # self.losses.append(loss.detach().cpu().item())\n",
    "        # ipdb.set_trace()\n",
    "        self.losses_actor.append(loss_actor.data.cpu().item())\n",
    "        self.losses_critic.append(loss_critic.data.cpu().item())\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        print(\"Going to be training for a total of {} episodes\".format(num_episodes))\n",
    "        state = Variable(torch.Tensor(self.env.reset()))\n",
    "        for e in range(num_episodes):\n",
    "            steps, rewards, log_probs, values = self.play_episode(e)\n",
    "            self.train_rewards.append(sum(rewards))\n",
    "            self.train_steps.append(steps)\n",
    "            self.optimize(rewards, log_probs,values)\n",
    "\n",
    "            if (e+1) % 100 == 0:\n",
    "                print(\"Episode: {}, reward: {}, steps: {}\".format(e+1, sum(rewards), steps))\n",
    "\n",
    "            # Freeze the current policy and test over 100 episodes\n",
    "            if (e+1) % self.test_freq == 0:\n",
    "                print(\"-\"*10 + \" testing now \" + \"-\"*10)\n",
    "                self.test(self.test_episodes, e)\n",
    "\n",
    "            # Save the current policy model\n",
    "            if (e+1) % (self.save_freq) == 0:\n",
    "                torch.save(self.actor.state_dict(),  os.path.join(self.save_path, \"train_actor_ep_{}.pkl\".format(e+1)))\n",
    "                torch.save(self.critic.state_dict(), os.path.join(self.save_path, \"train_critic_ep_{}.pkl\".format(e+1)))\n",
    "\n",
    "        # plot once when done training\n",
    "        self.plot_rewards(save=True)\n",
    "\n",
    "    def test(self, num_episodes, e_train):\n",
    "        state = Variable(torch.Tensor(self.env.reset()))\n",
    "        testing_rewards = []\n",
    "        testing_steps = []\n",
    "        for e in range(num_episodes):\n",
    "            steps, rewards, log_probs,values = self.play_episode(e)\n",
    "            self.test_rewards.append(sum(rewards))\n",
    "            self.test_steps.append(steps)\n",
    "            testing_rewards.append(sum(rewards))\n",
    "            testing_steps.append(steps)\n",
    "        print(\"Mean reward achieved : {} \".format(np.mean(testing_rewards)))\n",
    "        print(\"-\"*50)\n",
    "        if np.mean(testing_rewards) >= 200:\n",
    "            print(\"-\"*10 + \" Solved! \" + \"-\"*10)\n",
    "            print(\"Mean reward achieved : {} in {} steps\".format(np.mean(testing_rewards), np.mean(testing_steps)))\n",
    "            print(\"-\"*50)\n",
    "            # if (e_train+1) % 5000 == 0: self.plot_rewards(save=True)\n",
    "            # else: self.plot_rewards(save=False)\n",
    "        if (e_train+1) % 5000 == 0: self.plot_rewards(save=True)\n",
    "        #else: self.plot_rewards(save=False)\n",
    "\n",
    "    def plot_rewards(self, save=False):\n",
    "        train_rewards = [self.train_rewards[i:i+self.test_freq] for i in range(0,len(self.train_rewards),self.test_freq)]\n",
    "        test_rewards = [self.test_rewards[i:i+self.test_episodes] for i in range(0,len(self.test_rewards),self.test_episodes)]\n",
    "        train_losses_actor = [self.losses_actor[i:i+self.test_freq] for i in range(0,len(self.losses_actor),self.test_freq)]\n",
    "        train_losses_critic = [self.losses_critic[i:i+self.test_freq] for i in range(0,len(self.losses_critic),self.test_freq)]\n",
    "        train_losses = [self.losses_critic[i:i+self.test_freq]+self.losses_actor[i:i+self.test_freq] for i in range(0,len(self.losses_critic),self.test_freq)]\n",
    "\n",
    "        # rewards\n",
    "        train_rewards_mean = [np.mean(i) for i in train_rewards]\n",
    "        test_rewards_mean = [np.mean(i) for i in test_rewards]\n",
    "        train_rewards_std = [np.std(i) for i in train_rewards]\n",
    "        test_rewards_std = [np.std(i) for i in test_rewards]\n",
    "        train_nepisodes = [self.test_freq * (i+1) for i in range(len(train_rewards_mean))]\n",
    "\n",
    "        # steps\n",
    "        train_steps = [self.train_steps[i:i+self.test_freq] for i in range(0,len(self.train_steps),self.test_freq)]\n",
    "        test_steps = [self.test_steps[i:i+self.test_episodes] for i in range(0,len(self.test_steps),self.test_episodes)]\n",
    "        train_steps_mean = [np.mean(i) for i in train_steps]\n",
    "        test_steps_mean = [np.mean(i) for i in test_steps]\n",
    "        train_steps_std = [np.mean(i) for i in train_steps]\n",
    "        test_steps_std = [np.mean(i) for i in test_steps]\n",
    "\n",
    "        # loss\n",
    "        train_losses_actor_mean = [np.mean(i) for i in train_losses_actor]\n",
    "        train_losses_actor_std = [np.std(i) for i in train_losses_actor]\n",
    "        train_losses_critic_mean = [np.mean(i) for i in train_losses_critic]\n",
    "        train_losses_critic_std = [np.std(i) for i in train_losses_critic]\n",
    "        train_losses_mean = [np.mean(i) for i in train_losses]\n",
    "        train_losses_std = [np.std(i) for i in train_losses]\n",
    "\n",
    "\n",
    "        # training : reward over time\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        plt.title(\"Training : Avg. Reward over {} episodes\".format(self.test_episodes))\n",
    "        plt.xlabel(\"Number of training episodes\")\n",
    "        plt.ylabel(\"Avg Reward\")\n",
    "        plt.errorbar(train_nepisodes, train_rewards_mean, yerr=train_rewards_std, color=\"indigo\", uplims=True, lolims=True)\n",
    "        if save :\n",
    "            plt.savefig(self.expt_name + \"train_rewards_{}.png\".format(len(self.train_rewards)))\n",
    "        else:\n",
    "            plt.show()\n",
    "            # pause so that the plots are updated\n",
    "            plt.pause(0.001)\n",
    "\n",
    "        # testing : reward over time\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        plt.title(\"Testing : Avg. Reward over {} episodes\".format(self.test_episodes))\n",
    "        plt.xlabel(\"Number of training episodes\")\n",
    "        plt.ylabel(\"Avg Reward\")\n",
    "        try:\n",
    "            plt.errorbar(train_nepisodes, test_rewards_mean, yerr=test_rewards_std, color=\"indigo\", uplims=True, lolims=True)\n",
    "        except:\n",
    "            ipdb.set_trace()\n",
    "        if save :\n",
    "            plt.savefig(self.expt_name + \"test_rewards_{}.png\".format(len(self.test_rewards)))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        # training : avg number of steps per episode\n",
    "        plt.figure(3)\n",
    "        plt.clf()\n",
    "        plt.title(\"Training : Avg. number of steps taken per episode\")\n",
    "        plt.xlabel(\"Number of training episodes\")\n",
    "        plt.ylabel(\"Avg number of steps\")\n",
    "        plt.errorbar(train_nepisodes, train_steps_mean, yerr=train_steps_std, color=\"navy\", uplims=True, lolims=True)\n",
    "        if save :\n",
    "            plt.savefig(self.expt_name + \"train_steps_{}.png\".format(len(self.train_steps)))\n",
    "        else:\n",
    "            plt.show()\n",
    "            # pause so that the plots are updated\n",
    "            plt.pause(0.001)\n",
    "\n",
    "        # testing : avg number of steps per episode\n",
    "        plt.figure(4)\n",
    "        plt.clf()\n",
    "        plt.title(\"Testing : Avg. number of steps taken per episode\")\n",
    "        plt.xlabel(\"Number of training episodes\")\n",
    "        plt.ylabel(\"Avg number of steps\")\n",
    "        plt.errorbar(train_nepisodes, test_steps_mean, yerr=test_steps_std, color=\"navy\", uplims=True, lolims=True)\n",
    "        if save :\n",
    "            plt.savefig(self.expt_name + \"test_steps_{}.png\".format(len(self.test_steps)))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        # training : avg actor loss over time\n",
    "        plt.figure(5)\n",
    "        plt.clf()\n",
    "        plt.title(\"Avg. Actor Training Loss over {} episodes\".format(train_nepisodes[-1]))\n",
    "        plt.xlabel(\"Number of training episodes\")\n",
    "        plt.ylabel(\"Avg. Loss\")\n",
    "        # plt.plot(train_losses_mean, color=\"crimson\")\n",
    "        plt.errorbar(train_nepisodes, train_losses_mean, yerr=train_losses_std, color=\"tomato\", uplims=True, lolims=True)\n",
    "        if save :\n",
    "            plt.savefig(self.expt_name + \"train_loss_actor_{}.png\".format(len(self.test_rewards)))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        # training : avg critic loss over time\n",
    "        plt.figure(5)\n",
    "        plt.clf()\n",
    "        plt.title(\"Avg. Critic Training Loss over {} episodes\".format(train_nepisodes[-1]))\n",
    "        plt.xlabel(\"Number of training episodes\")\n",
    "        plt.ylabel(\"Avg. Loss\")\n",
    "        # plt.plot(train_losses_mean, color=\"crimson\")\n",
    "        plt.errorbar(train_nepisodes, train_losses_mean, yerr=train_losses_std, color=\"tomato\", uplims=True, lolims=True)\n",
    "        if save :\n",
    "            plt.savefig(self.expt_name + \"train_loss_critic_{}.png\".format(len(self.test_rewards)))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "        # training : combined avg loss over time\n",
    "        plt.figure(5)\n",
    "        plt.clf()\n",
    "        plt.title(\"Avg. AC Combined Training Loss over {} episodes\".format(train_nepisodes[-1]))\n",
    "        plt.xlabel(\"Number of training episodes\")\n",
    "        plt.ylabel(\"Avg. Loss\")\n",
    "        # plt.plot(train_losses_mean, color=\"crimson\")\n",
    "        plt.errorbar(train_nepisodes, train_losses_mean, yerr=train_losses_std, color=\"crimson\", uplims=True, lolims=True)\n",
    "        if save :\n",
    "            plt.savefig(self.expt_name + \"train_loss_{}.png\".format(len(self.test_rewards)))\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.gamma=0.99\n",
    "        self.lr_actor=5e-4\n",
    "        self.lr_critic=5e-4\n",
    "        self.num_episodes=50000\n",
    "        self.test_episodes=100\n",
    "        self.num_steps=50\n",
    "        self.seed=123\n",
    "        self.save_freq=1e4\n",
    "        self.test_freq=500\n",
    "        self.save_path=\"models/a2c/\"\n",
    "        self.expt_name=\"plots/a2c/\"\n",
    "        self.N_steps=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "args = Parameters()\n",
    "    \n",
    "args.gamma=0.99\n",
    "args.lr_actor=1e-3\n",
    "args.lr_critic=1e-3\n",
    "args.num_episodes=6000\n",
    "args.test_episodes=100\n",
    "args.num_steps=50\n",
    "args.seed=42\n",
    "args.save_freq=1e3\n",
    "args.test_freq=500\n",
    "args.save_path=\"models/a2c/\"\n",
    "args.expt_name=\"plots/a2c/\"\n",
    "args.N_steps=100\n",
    "\n",
    "\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.mkdir(args.save_path)\n",
    "if not os.path.exists(args.expt_name):\n",
    "    os.mkdir(args.expt_name)\n",
    "\n",
    "    # create the environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.seed(42)\n",
    "torch.manual_seed(42)\n",
    "    # plt.ion()\n",
    "\n",
    "    # A2C agent\n",
    "agent = A2C(env, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to be training for a total of 6000 episodes\n"
     ]
    }
   ],
   "source": [
    "agent.train(args.num_episodes)\n",
    "# agent.test()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(actor,critic, state):\n",
    "    state = Variable(Tensor(state))\n",
    "    log_probs = actor(state)\n",
    "    value = critic(state)\n",
    "    action = Categorical(log_probs.exp()).sample()\n",
    "    return action.data.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = agent.actor\n",
    "actor.load_state_dict(torch.load('models/a2c/train_actor_ep_4000.pkl'))\n",
    "critic = agent.critic\n",
    "critic.load_state_dict(torch.load('models/a2c/train_critic_ep_4000.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.007691021764686\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tsteps=0\n",
    "treward=0\n",
    "frames = []\n",
    "env.close()\n",
    "for nexps in range(3):   # Let's do 10 trials\n",
    "    done= False  \n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = select_action(actor, critic, observation)\n",
    "#         action, _, _ = agent.select_action(observation)\n",
    "        observation, reward, done, info = env.step(action) # take action from learned policy\n",
    "        treward = treward + reward\n",
    "        tsteps = tsteps + 1\n",
    "env.close()\n",
    "print(treward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_arrays(matrix, PATH):\n",
    "    np.savetxt(PATH, np.array(matrix) , fmt='%f')\n",
    "\n",
    "def load_arrays(PATH):\n",
    "    return np.loadtxt(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gonzalo/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "plt.plot(agent.train_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent.train_rewards)\n",
    "save_arrays(agent.train_rewards, 'results/a2c/rewards_1.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
